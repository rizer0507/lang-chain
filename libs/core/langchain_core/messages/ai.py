"""AI message.

中文翻译:
人工智能消息。"""

import itertools
import json
import logging
import operator
from collections.abc import Sequence
from typing import Any, Literal, cast, overload

from pydantic import Field, model_validator
from typing_extensions import NotRequired, Self, TypedDict, override

from langchain_core.messages import content as types
from langchain_core.messages.base import (
    BaseMessage,
    BaseMessageChunk,
    _extract_reasoning_from_additional_kwargs,
    merge_content,
)
from langchain_core.messages.content import InvalidToolCall
from langchain_core.messages.tool import (
    ToolCall,
    ToolCallChunk,
    default_tool_chunk_parser,
    default_tool_parser,
)
from langchain_core.messages.tool import invalid_tool_call as create_invalid_tool_call
from langchain_core.messages.tool import tool_call as create_tool_call
from langchain_core.messages.tool import tool_call_chunk as create_tool_call_chunk
from langchain_core.utils._merge import merge_dicts, merge_lists
from langchain_core.utils.json import parse_partial_json
from langchain_core.utils.usage import _dict_int_op
from langchain_core.utils.utils import LC_AUTO_PREFIX, LC_ID_PREFIX

logger = logging.getLogger(__name__)


class InputTokenDetails(TypedDict, total=False):
    """Breakdown of input token counts.

    Does *not* need to sum to full input token count. Does *not* need to have all keys.

    Example:
        ```python
        {
            "audio": 10,
            "cache_creation": 200,
            "cache_read": 100,
        }
        ```

    May also hold extra provider-specific keys.

    !!! version-added "Added in `langchain-core` 0.3.9"
    

    中文翻译:
    输入令牌计数的细分。
    *不需要*需要对完整的输入令牌计数进行求和。 *不需要*需要拥有所有钥匙。
    示例：
        ````蟒蛇
        {
            “音频”：10，
            “缓存创建”：200，
            “缓存读取”：100，
        }
        ````
    还可以保存额外的特定于提供商的密钥。
    ！！！ version-added “在 `langchain-core` 0.3.9 中添加”"""

    audio: int
    """Audio input tokens.

    中文翻译:
    音频输入令牌。"""

    cache_creation: int
    """Input tokens that were cached and there was a cache miss.

    Since there was a cache miss, the cache was created from these tokens.
    

    中文翻译:
    已缓存的输入令牌并且存在缓存未命中。
    由于存在缓存未命中，因此缓存是根据这些令牌创建的。"""

    cache_read: int
    """Input tokens that were cached and there was a cache hit.

    Since there was a cache hit, the tokens were read from the cache. More precisely,
    the model state given these tokens was read from the cache.
    

    中文翻译:
    已缓存的输入令牌并且存在缓存命中。
    由于存在缓存命中，因此从缓存中读取令牌。更准确地说，
    给定这些标记的模型状态是从缓存中读取的。"""


class OutputTokenDetails(TypedDict, total=False):
    """Breakdown of output token counts.

    Does *not* need to sum to full output token count. Does *not* need to have all keys.

    Example:
        ```python
        {
            "audio": 10,
            "reasoning": 200,
        }
        ```

    May also hold extra provider-specific keys.

    !!! version-added "Added in `langchain-core` 0.3.9"

    

    中文翻译:
    输出令牌计数的细分。
    *不需要*需要将总输出令牌计数相加。 *不需要*需要拥有所有钥匙。
    示例：
        ````蟒蛇
        {
            “音频”：10，
            “推理”：200，
        }
        ````
    还可以保存额外的特定于提供商的密钥。
    !!! version-added “在 `langchain-core` 0.3.9 中添加”"""

    audio: int
    """Audio output tokens.

    中文翻译:
    音频输出令牌。"""

    reasoning: int
    """Reasoning output tokens.

    Tokens generated by the model in a chain of thought process (i.e. by OpenAI's o1
    models) that are not returned as part of model output.
    

    中文翻译:
    推理输出标记。
    模型在一系列思维过程中生成的代币（即由 OpenAI 的 o1
    模型）不会作为模型输出的一部分返回。"""


class UsageMetadata(TypedDict):
    """Usage metadata for a message, such as token counts.

    This is a standard representation of token usage that is consistent across models.

    Example:
        ```python
        {
            "input_tokens": 350,
            "output_tokens": 240,
            "total_tokens": 590,
            "input_token_details": {
                "audio": 10,
                "cache_creation": 200,
                "cache_read": 100,
            },
            "output_token_details": {
                "audio": 10,
                "reasoning": 200,
            },
        }
        ```

    !!! warning "Behavior changed in `langchain-core` 0.3.9"

        Added `input_token_details` and `output_token_details`.

    !!! note "LangSmith SDK"

        The LangSmith SDK also has a `UsageMetadata` class. While the two share fields,
        LangSmith's `UsageMetadata` has additional fields to capture cost information
        used by the LangSmith platform.
    

    中文翻译:
    消息的使用元数据，例如令牌计数。
    这是令牌使用的标准表示，在各个模型中保持一致。
    示例：
        ````蟒蛇
        {
            “输入令牌”：350，
            “输出令牌”：240，
            “total_tokens”：590，
            “输入令牌详细信息”：{
                “音频”：10，
                “缓存创建”：200，
                “缓存读取”：100，
            },
            “输出令牌详细信息”：{
                “音频”：10，
                “推理”：200，
            },
        }
        ````
    ！！！警告“`langchain-core` 0.3.9 中的行为已更改”
        添加了“input_token_details”和“output_token_details”。
    ！！！注意“LangSmith SDK”
        LangSmith SDK 还有一个“UsageMetadata”类。虽然两人共享领域，
        LangSmith 的 `UsageMetadata` 有额外的字段来捕获成本信息
        由 LangSmith 平台使用。"""

    input_tokens: int
    """Count of input (or prompt) tokens. Sum of all input token types.

    中文翻译:
    输入（或提示）标记的计数。所有输入标记类型的总和。"""

    output_tokens: int
    """Count of output (or completion) tokens. Sum of all output token types.

    中文翻译:
    输出（或完成）标记的计数。所有输出令牌类型的总和。"""

    total_tokens: int
    """Total token count. Sum of `input_tokens` + `output_tokens`.

    中文翻译:
    令牌总数。 `input_tokens` + `output_tokens` 的总和。"""

    input_token_details: NotRequired[InputTokenDetails]
    """Breakdown of input token counts.

    Does *not* need to sum to full input token count. Does *not* need to have all keys.
    

    中文翻译:
    输入令牌计数的细分。
    *不需要*需要对完整的输入令牌计数进行求和。 *不需要*需要拥有所有钥匙。"""

    output_token_details: NotRequired[OutputTokenDetails]
    """Breakdown of output token counts.

    Does *not* need to sum to full output token count. Does *not* need to have all keys.
    

    中文翻译:
    输出令牌计数的细分。
    *不需要*需要将总输出令牌计数相加。 *不需要*需要拥有所有钥匙。"""


class AIMessage(BaseMessage):
    """Message from an AI.

    An `AIMessage` is returned from a chat model as a response to a prompt.

    This message represents the output of the model and consists of both
    the raw output as returned by the model and standardized fields
    (e.g., tool calls, usage metadata) added by the LangChain framework.
    

    中文翻译:
    来自人工智能的消息。
    聊天模型返回“AIMessage”作为对提示的响应。
    该消息代表模型的输出，由以下两部分组成：
    模型和标准化字段返回的原始输出
    （例如，工具调用、使用元数据）由LangChain框架添加。"""

    tool_calls: list[ToolCall] = Field(default_factory=list)
    """If present, tool calls associated with the message.

    中文翻译:
    如果存在，则调用与该消息关联的工具。"""

    invalid_tool_calls: list[InvalidToolCall] = Field(default_factory=list)
    """If present, tool calls with parsing errors associated with the message.

    中文翻译:
    如果存在，工具调用会出现与消息相关的解析错误。"""

    usage_metadata: UsageMetadata | None = None
    """If present, usage metadata for a message, such as token counts.

    This is a standard representation of token usage that is consistent across models.
    

    中文翻译:
    如果存在，则消息的使用元数据，例如令牌计数。
    这是令牌使用的标准表示，在各个模型中保持一致。"""

    type: Literal["ai"] = "ai"
    """The type of the message (used for deserialization).

    中文翻译:
    消息的类型（用于反序列化）。"""

    @overload
    def __init__(
        self,
        content: str | list[str | dict],
        **kwargs: Any,
    ) -> None: ...

    @overload
    def __init__(
        self,
        content: str | list[str | dict] | None = None,
        content_blocks: list[types.ContentBlock] | None = None,
        **kwargs: Any,
    ) -> None: ...

    def __init__(
        self,
        content: str | list[str | dict] | None = None,
        content_blocks: list[types.ContentBlock] | None = None,
        **kwargs: Any,
    ) -> None:
        """Initialize an `AIMessage`.

        Specify `content` as positional arg or `content_blocks` for typing.

        Args:
            content: The content of the message.
            content_blocks: Typed standard content.
            **kwargs: Additional arguments to pass to the parent class.
        

        中文翻译:
        初始化一个“AIMessage”。
        将 `content` 指定为位置参数或用于输入的 `content_blocks`。
        参数：
            内容：消息的内容。
            content_blocks：键入的标准内容。
            **kwargs：传递给父类的附加参数。"""
        if content_blocks is not None:
            # If there are tool calls in content_blocks, but not in tool_calls, add them
            # 中文: 如果 content_blocks 中有工具调用，但 tool_calls 中没有，则添加它们
            content_tool_calls = [
                block for block in content_blocks if block.get("type") == "tool_call"
            ]
            if content_tool_calls and "tool_calls" not in kwargs:
                kwargs["tool_calls"] = content_tool_calls

            super().__init__(
                content=cast("str | list[str | dict]", content_blocks),
                **kwargs,
            )
        else:
            super().__init__(content=content, **kwargs)

    @property
    def lc_attributes(self) -> dict:
        """Attributes to be serialized.

        Includes all attributes, even if they are derived from other initialization
        arguments.
        

        中文翻译:
        要序列化的属性。
        包括所有属性，即使它们是从其他初始化派生的
        论据。"""
        return {
            "tool_calls": self.tool_calls,
            "invalid_tool_calls": self.invalid_tool_calls,
        }

    @property
    def content_blocks(self) -> list[types.ContentBlock]:
        """Return standard, typed `ContentBlock` dicts from the message.

        If the message has a known model provider, use the provider-specific translator
        first before falling back to best-effort parsing. For details, see the property
        on `BaseMessage`.
        

        中文翻译:
        从消息中返回标准的、键入的“ContentBlock”字典。
        如果消息有已知的模型提供者，则使用提供者特定的转换器
        首先，然后再回到尽力解析。详情请参阅楼盘
        在“BaseMessage”上。"""
        if self.response_metadata.get("output_version") == "v1":
            return cast("list[types.ContentBlock]", self.content)

        model_provider = self.response_metadata.get("model_provider")
        if model_provider:
            from langchain_core.messages.block_translators import (  # noqa: PLC0415
                get_translator,
            )

            translator = get_translator(model_provider)
            if translator:
                try:
                    return translator["translate_content"](self)
                except NotImplementedError:
                    pass

        # Otherwise, use best-effort parsing
        # 中文: 否则，使用尽力解析
        blocks = super().content_blocks

        if self.tool_calls:
            # Add from tool_calls if missing from content
            # 中文: 如果内容缺失，请从 tool_calls 添加
            content_tool_call_ids = {
                block.get("id")
                for block in self.content
                if isinstance(block, dict) and block.get("type") == "tool_call"
            }
            for tool_call in self.tool_calls:
                if (id_ := tool_call.get("id")) and id_ not in content_tool_call_ids:
                    tool_call_block: types.ToolCall = {
                        "type": "tool_call",
                        "id": id_,
                        "name": tool_call["name"],
                        "args": tool_call["args"],
                    }
                    if "index" in tool_call:
                        tool_call_block["index"] = tool_call["index"]  # type: ignore[typeddict-item]
                    if "extras" in tool_call:
                        tool_call_block["extras"] = tool_call["extras"]  # type: ignore[typeddict-item]
                    blocks.append(tool_call_block)

        # Best-effort reasoning extraction from additional_kwargs
        # 中文: 从additional_kwargs 中进行尽力推理提取
        # Only add reasoning if not already present
        # 中文: 仅添加尚不存在的推理
        # Insert before all other blocks to keep reasoning at the start
        # 中文: 在所有其他块之前插入以在开始时保持推理
        has_reasoning = any(block.get("type") == "reasoning" for block in blocks)
        if not has_reasoning and (
            reasoning_block := _extract_reasoning_from_additional_kwargs(self)
        ):
            blocks.insert(0, reasoning_block)

        return blocks

    # TODO: remove this logic if possible, reducing breaking nature of changes
    @model_validator(mode="before")
    @classmethod
    def _backwards_compat_tool_calls(cls, values: dict) -> Any:
        check_additional_kwargs = not any(
            values.get(k)
            for k in ("tool_calls", "invalid_tool_calls", "tool_call_chunks")
        )
        if check_additional_kwargs and (
            raw_tool_calls := values.get("additional_kwargs", {}).get("tool_calls")
        ):
            try:
                if issubclass(cls, AIMessageChunk):
                    values["tool_call_chunks"] = default_tool_chunk_parser(
                        raw_tool_calls
                    )
                else:
                    parsed_tool_calls, parsed_invalid_tool_calls = default_tool_parser(
                        raw_tool_calls
                    )
                    values["tool_calls"] = parsed_tool_calls
                    values["invalid_tool_calls"] = parsed_invalid_tool_calls
            except Exception:
                logger.debug("Failed to parse tool calls", exc_info=True)

        # Ensure "type" is properly set on all tool call-like dicts.
        # 中文: 确保在所有类似工具调用的字典上正确设置“类型”。
        if tool_calls := values.get("tool_calls"):
            values["tool_calls"] = [
                create_tool_call(
                    **{k: v for k, v in tc.items() if k not in {"type", "extras"}}
                )
                for tc in tool_calls
            ]
        if invalid_tool_calls := values.get("invalid_tool_calls"):
            values["invalid_tool_calls"] = [
                create_invalid_tool_call(**{k: v for k, v in tc.items() if k != "type"})
                for tc in invalid_tool_calls
            ]

        if tool_call_chunks := values.get("tool_call_chunks"):
            values["tool_call_chunks"] = [
                create_tool_call_chunk(**{k: v for k, v in tc.items() if k != "type"})
                for tc in tool_call_chunks
            ]

        return values

    @override
    def pretty_repr(self, html: bool = False) -> str:
        """Return a pretty representation of the message for display.

        Args:
            html: Whether to return an HTML-formatted string.

        Returns:
            A pretty representation of the message.

        

        中文翻译:
        返回消息的漂亮表示以供显示。
        参数：
            html：是否返回HTML格式的字符串。
        返回：
            很好地表达了该消息。"""
        base = super().pretty_repr(html=html)
        lines = []

        def _format_tool_args(tc: ToolCall | InvalidToolCall) -> list[str]:
            lines = [
                f"  {tc.get('name', 'Tool')} ({tc.get('id')})",
                f" Call ID: {tc.get('id')}",
            ]
            if tc.get("error"):
                lines.append(f"  Error: {tc.get('error')}")
            lines.append("  Args:")
            args = tc.get("args")
            if isinstance(args, str):
                lines.append(f"    {args}")
            elif isinstance(args, dict):
                for arg, value in args.items():
                    lines.append(f"    {arg}: {value}")
            return lines

        if self.tool_calls:
            lines.append("Tool Calls:")
            for tc in self.tool_calls:
                lines.extend(_format_tool_args(tc))
        if self.invalid_tool_calls:
            lines.append("Invalid Tool Calls:")
            for itc in self.invalid_tool_calls:
                lines.extend(_format_tool_args(itc))
        return (base.strip() + "\n" + "\n".join(lines)).strip()


class AIMessageChunk(AIMessage, BaseMessageChunk):
    """Message chunk from an AI (yielded when streaming).

    中文翻译:
    来自 AI 的消息块（流式传输时产生）。"""

    # Ignoring mypy re-assignment here since we're overriding the value
    # 中文: 此处忽略 mypy 重新分配，因为我们要覆盖该值
    # to make sure that the chunk variant can be discriminated from the
    # 中文: 以确保可以将块变体与
    # non-chunk variant.
    # 中文: 非块变体。
    type: Literal["AIMessageChunk"] = "AIMessageChunk"  # type: ignore[assignment]
    """The type of the message (used for deserialization).

    中文翻译:
    消息的类型（用于反序列化）。"""

    tool_call_chunks: list[ToolCallChunk] = Field(default_factory=list)
    """If provided, tool call chunks associated with the message.

    中文翻译:
    如果提供，工具调用与消息关联的块。"""

    chunk_position: Literal["last"] | None = None
    """Optional span represented by an aggregated `AIMessageChunk`.

    If a chunk with `chunk_position="last"` is aggregated into a stream,
    `tool_call_chunks` in message content will be parsed into `tool_calls`.
    

    中文翻译:
    由聚合的“AIMessageChunk”表示的可选范围。
    如果一个带有 `chunk_position="last"` 的块被聚合到一个流中，
    消息内容中的`tool_call_chunks`将被解析为`tool_calls`。"""

    @property
    def lc_attributes(self) -> dict:
        """Attributes to be serialized, even if they are derived from other initialization args.

        中文翻译:
        要序列化的属性，即使它们是从其他初始化参数派生的。"""  # noqa: E501
        return {
            "tool_calls": self.tool_calls,
            "invalid_tool_calls": self.invalid_tool_calls,
        }

    @property
    def content_blocks(self) -> list[types.ContentBlock]:
        """Return standard, typed `ContentBlock` dicts from the message.

        中文翻译:
        从消息中返回标准的、键入的“ContentBlock”字典。"""
        if self.response_metadata.get("output_version") == "v1":
            return cast("list[types.ContentBlock]", self.content)

        model_provider = self.response_metadata.get("model_provider")
        if model_provider:
            from langchain_core.messages.block_translators import (  # noqa: PLC0415
                get_translator,
            )

            translator = get_translator(model_provider)
            if translator:
                try:
                    return translator["translate_content_chunk"](self)
                except NotImplementedError:
                    pass

        # Otherwise, use best-effort parsing
        # 中文: 否则，使用尽力解析
        blocks = super().content_blocks

        if (
            self.tool_call_chunks
            and not self.content
            and self.chunk_position != "last"  # keep tool_calls if aggregated
        ):
            blocks = [
                block
                for block in blocks
                if block["type"] not in {"tool_call", "invalid_tool_call"}
            ]
            for tool_call_chunk in self.tool_call_chunks:
                tc: types.ToolCallChunk = {
                    "type": "tool_call_chunk",
                    "id": tool_call_chunk.get("id"),
                    "name": tool_call_chunk.get("name"),
                    "args": tool_call_chunk.get("args"),
                }
                if (idx := tool_call_chunk.get("index")) is not None:
                    tc["index"] = idx
                blocks.append(tc)

        # Best-effort reasoning extraction from additional_kwargs
        # 中文: 从additional_kwargs 中进行尽力推理提取
        # Only add reasoning if not already present
        # 中文: 仅添加尚不存在的推理
        # Insert before all other blocks to keep reasoning at the start
        # 中文: 在所有其他块之前插入以在开始时保持推理
        has_reasoning = any(block.get("type") == "reasoning" for block in blocks)
        if not has_reasoning and (
            reasoning_block := _extract_reasoning_from_additional_kwargs(self)
        ):
            blocks.insert(0, reasoning_block)

        return blocks

    @model_validator(mode="after")
    def init_tool_calls(self) -> Self:
        """Initialize tool calls from tool call chunks.

        Returns:
            The values with tool calls initialized.

        Raises:
            ValueError: If the tool call chunks are malformed.
        

        中文翻译:
        从工具调用块初始化工具调用。
        返回：
            工具调用已初始化的值。
        加薪：
            ValueError：如果工具调用块格式错误。"""
        if not self.tool_call_chunks:
            if self.tool_calls:
                self.tool_call_chunks = [
                    create_tool_call_chunk(
                        name=tc["name"],
                        args=json.dumps(tc["args"]),
                        id=tc["id"],
                        index=None,
                    )
                    for tc in self.tool_calls
                ]
            if self.invalid_tool_calls:
                tool_call_chunks = self.tool_call_chunks
                tool_call_chunks.extend(
                    [
                        create_tool_call_chunk(
                            name=tc["name"], args=tc["args"], id=tc["id"], index=None
                        )
                        for tc in self.invalid_tool_calls
                    ]
                )
                self.tool_call_chunks = tool_call_chunks

            return self
        tool_calls = []
        invalid_tool_calls = []

        def add_chunk_to_invalid_tool_calls(chunk: ToolCallChunk) -> None:
            invalid_tool_calls.append(
                create_invalid_tool_call(
                    name=chunk["name"],
                    args=chunk["args"],
                    id=chunk["id"],
                    error=None,
                )
            )

        for chunk in self.tool_call_chunks:
            try:
                args_ = parse_partial_json(chunk["args"]) if chunk["args"] else {}
                if isinstance(args_, dict):
                    tool_calls.append(
                        create_tool_call(
                            name=chunk["name"] or "",
                            args=args_,
                            id=chunk["id"],
                        )
                    )
                else:
                    add_chunk_to_invalid_tool_calls(chunk)
            except Exception:
                add_chunk_to_invalid_tool_calls(chunk)
        self.tool_calls = tool_calls
        self.invalid_tool_calls = invalid_tool_calls

        if (
            self.chunk_position == "last"
            and self.tool_call_chunks
            and self.response_metadata.get("output_version") == "v1"
            and isinstance(self.content, list)
        ):
            id_to_tc: dict[str, types.ToolCall] = {
                cast("str", tc.get("id")): {
                    "type": "tool_call",
                    "name": tc["name"],
                    "args": tc["args"],
                    "id": tc.get("id"),
                }
                for tc in self.tool_calls
                if "id" in tc
            }
            for idx, block in enumerate(self.content):
                if (
                    isinstance(block, dict)
                    and block.get("type") == "tool_call_chunk"
                    and (call_id := block.get("id"))
                    and call_id in id_to_tc
                ):
                    self.content[idx] = cast("dict[str, Any]", id_to_tc[call_id])
                    if "extras" in block:
                        # mypy does not account for instance check for dict above
                        # 中文: mypy 不考虑例如检查上面的 dict
                        self.content[idx]["extras"] = block["extras"]  # type: ignore[index]

        return self

    @model_validator(mode="after")
    def init_server_tool_calls(self) -> Self:
        """Parse `server_tool_call_chunks` from [`ServerToolCallChunk`][langchain.messages.ServerToolCallChunk] objects.

        中文翻译:
        从 [`ServerToolCallChunk`][langchain.messages.ServerToolCallChunk] 对象中解析 `server_tool_call_chunks`。"""  # noqa: E501
        if (
            self.chunk_position == "last"
            and self.response_metadata.get("output_version") == "v1"
            and isinstance(self.content, list)
        ):
            for idx, block in enumerate(self.content):
                if (
                    isinstance(block, dict)
                    and block.get("type")
                    in {"server_tool_call", "server_tool_call_chunk"}
                    and (args_str := block.get("args"))
                    and isinstance(args_str, str)
                ):
                    try:
                        args = json.loads(args_str)
                        if isinstance(args, dict):
                            self.content[idx]["type"] = "server_tool_call"  # type: ignore[index]
                            self.content[idx]["args"] = args  # type: ignore[index]
                    except json.JSONDecodeError:
                        pass
        return self

    @overload  # type: ignore[override]  # summing BaseMessages gives ChatPromptTemplate
    def __add__(self, other: "AIMessageChunk") -> "AIMessageChunk": ...

    @overload
    def __add__(self, other: Sequence["AIMessageChunk"]) -> "AIMessageChunk": ...

    @overload
    def __add__(self, other: Any) -> BaseMessageChunk: ...

    @override
    def __add__(self, other: Any) -> BaseMessageChunk:
        if isinstance(other, AIMessageChunk):
            return add_ai_message_chunks(self, other)
        if isinstance(other, (list, tuple)) and all(
            isinstance(o, AIMessageChunk) for o in other
        ):
            return add_ai_message_chunks(self, *other)
        return super().__add__(other)


def add_ai_message_chunks(
    left: AIMessageChunk, *others: AIMessageChunk
) -> AIMessageChunk:
    """Add multiple `AIMessageChunk`s together.

    Args:
        left: The first `AIMessageChunk`.
        *others: Other `AIMessageChunk`s to add.

    Returns:
        The resulting `AIMessageChunk`.

    

    中文翻译:
    将多个“AIMessageChunk”添加在一起。
    参数：
        左：第一个“AIMessageChunk”。
        *其他：要添加的其他“AIMessageChunk”。
    返回：
        生成的“AIMessageChunk”。"""
    content = merge_content(left.content, *(o.content for o in others))
    additional_kwargs = merge_dicts(
        left.additional_kwargs, *(o.additional_kwargs for o in others)
    )
    response_metadata = merge_dicts(
        left.response_metadata, *(o.response_metadata for o in others)
    )

    # Merge tool call chunks
    # 中文: 合并工具调用块
    if raw_tool_calls := merge_lists(
        left.tool_call_chunks, *(o.tool_call_chunks for o in others)
    ):
        tool_call_chunks = [
            create_tool_call_chunk(
                name=rtc.get("name"),
                args=rtc.get("args"),
                index=rtc.get("index"),
                id=rtc.get("id"),
            )
            for rtc in raw_tool_calls
        ]
    else:
        tool_call_chunks = []

    # Token usage
    # 中文: 代币使用
    if left.usage_metadata or any(o.usage_metadata is not None for o in others):
        usage_metadata: UsageMetadata | None = left.usage_metadata
        for other in others:
            usage_metadata = add_usage(usage_metadata, other.usage_metadata)
    else:
        usage_metadata = None

    # Ranks are defined by the order of preference. Higher is better:
    # 中文: 排名由偏好顺序定义。越高越好：
    # 2. Provider-assigned IDs (non lc_* and non lc_run-*)
    # 中文: 2. 提供商分配的 ID（非 lc_* 和非 lc_run-*）
    # 1. lc_run-* IDs
    # 中文: 1. lc_run-* ID
    # 0. lc_* and other remaining IDs
    # 中文: 0. lc_*及其他剩余ID
    best_rank = -1
    chunk_id = None
    candidates = itertools.chain([left.id], (o.id for o in others))

    for id_ in candidates:
        if not id_:
            continue

        if not id_.startswith(LC_ID_PREFIX) and not id_.startswith(LC_AUTO_PREFIX):
            chunk_id = id_
            # Highest rank, return instantly
            # 中文: 最高排名，立即返回
            break

        rank = 1 if id_.startswith(LC_ID_PREFIX) else 0

        if rank > best_rank:
            best_rank = rank
            chunk_id = id_

    chunk_position: Literal["last"] | None = (
        "last" if any(x.chunk_position == "last" for x in [left, *others]) else None
    )

    return left.__class__(
        content=content,
        additional_kwargs=additional_kwargs,
        tool_call_chunks=tool_call_chunks,
        response_metadata=response_metadata,
        usage_metadata=usage_metadata,
        id=chunk_id,
        chunk_position=chunk_position,
    )


def add_usage(left: UsageMetadata | None, right: UsageMetadata | None) -> UsageMetadata:
    """Recursively add two UsageMetadata objects.

    Example:
        ```python
        from langchain_core.messages.ai import add_usage

        left = UsageMetadata(
            input_tokens=5,
            output_tokens=0,
            total_tokens=5,
            input_token_details=InputTokenDetails(cache_read=3),
        )
        right = UsageMetadata(
            input_tokens=0,
            output_tokens=10,
            total_tokens=10,
            output_token_details=OutputTokenDetails(reasoning=4),
        )

        add_usage(left, right)
        ```

        results in

        ```python
        UsageMetadata(
            input_tokens=5,
            output_tokens=10,
            total_tokens=15,
            input_token_details=InputTokenDetails(cache_read=3),
            output_token_details=OutputTokenDetails(reasoning=4),
        )
        ```
    Args:
        left: The first `UsageMetadata` object.
        right: The second `UsageMetadata` object.

    Returns:
        The sum of the two `UsageMetadata` objects.

    

    中文翻译:
    递归添加两个UsageMetadata 对象。
    示例：
        ````蟒蛇
        从 langchain_core.messages.ai 导入 add_usage
        左 = 使用元数据（
            输入令牌=5，
            输出令牌=0，
            总代币=5,
            input_token_details=InputTokenDetails(cache_read=3),
        ）
        右 = 使用元数据（
            输入令牌=0，
            输出令牌=10，
            总代币=10，
            output_token_details=OutputTokenDetails(推理=4),
        ）
        添加用法（左，右）
        ````
        结果
        ````蟒蛇
        使用元数据(
            输入令牌=5，
            输出令牌=10，
            总代币=15，
            input_token_details=InputTokenDetails(cache_read=3),
            output_token_details=OutputTokenDetails(推理=4),
        ）
        ````
    参数：
        左：第一个“UsageMetadata”对象。
        右：第二个“UsageMetadata”对象。
    返回：
        两个“UsageMetadata”对象的总和。"""
    if not (left or right):
        return UsageMetadata(input_tokens=0, output_tokens=0, total_tokens=0)
    if not (left and right):
        return cast("UsageMetadata", left or right)

    return UsageMetadata(
        **cast(
            "UsageMetadata",
            _dict_int_op(
                cast("dict", left),
                cast("dict", right),
                operator.add,
            ),
        )
    )


def subtract_usage(
    left: UsageMetadata | None, right: UsageMetadata | None
) -> UsageMetadata:
    """Recursively subtract two `UsageMetadata` objects.

    Token counts cannot be negative so the actual operation is `max(left - right, 0)`.

    Example:
        ```python
        from langchain_core.messages.ai import subtract_usage

        left = UsageMetadata(
            input_tokens=5,
            output_tokens=10,
            total_tokens=15,
            input_token_details=InputTokenDetails(cache_read=4),
        )
        right = UsageMetadata(
            input_tokens=3,
            output_tokens=8,
            total_tokens=11,
            output_token_details=OutputTokenDetails(reasoning=4),
        )

        subtract_usage(left, right)
        ```

        results in

        ```python
        UsageMetadata(
            input_tokens=2,
            output_tokens=2,
            total_tokens=4,
            input_token_details=InputTokenDetails(cache_read=4),
            output_token_details=OutputTokenDetails(reasoning=0),
        )
        ```
    Args:
        left: The first `UsageMetadata` object.
        right: The second `UsageMetadata` object.

    Returns:
        The resulting `UsageMetadata` after subtraction.

    

    中文翻译:
    递归地减去两个 `UsageMetadata` 对象。
    令牌计数不能为负，因此实际操作是“max(left - right, 0)”。
    示例：
        ````蟒蛇
        从langchain_core.messages.ai导入subtract_usage
        左 = 使用元数据（
            输入令牌=5，
            输出令牌=10，
            总代币=15，
            input_token_details=InputTokenDetails(cache_read=4),
        ）
        右 = 使用元数据（
            输入令牌=3，
            输出令牌=8，
            总代币=11，
            output_token_details=OutputTokenDetails(推理=4),
        ）
        减法用法（左、右）
        ````
        结果
        ````蟒蛇
        使用元数据(
            输入令牌=2，
            输出令牌=2，
            总代币数=4,
            input_token_details=InputTokenDetails(cache_read=4),
            output_token_details=OutputTokenDetails(推理=0),
        ）
        ````
    参数：
        左：第一个“UsageMetadata”对象。
        右：第二个“UsageMetadata”对象。
    返回：
        减去后得到的“UsageMetadata”。"""
    if not (left or right):
        return UsageMetadata(input_tokens=0, output_tokens=0, total_tokens=0)
    if not (left and right):
        return cast("UsageMetadata", left or right)

    return UsageMetadata(
        **cast(
            "UsageMetadata",
            _dict_int_op(
                cast("dict", left),
                cast("dict", right),
                (lambda le, ri: max(le - ri, 0)),
            ),
        )
    )
