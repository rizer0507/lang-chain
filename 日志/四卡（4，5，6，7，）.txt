(APIServer pid=1) INFO 01-16 17:28:26 [chat_utils.py:560] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
(APIServer pid=1) INFO 01-16 17:28:26 [logger.py:40] Received request chatcmpl-5fa9becc620f40588a1cee41abaddde8: prompt: '<|im_start|>system\n你是一个有用的助手。你会通过 message_save 工具保存答案到文件。\n\n工作流程：\n1. 当你第一次回答用户问题时，生成完整的中文答案，并将其作为 prompt 参数传递给 message_save 工具\n2. message_save 工具会将答案保存到文件并返回确认信息\n3. 看到工具返回后，将同样的答案直接输出给用户\n\n注意：\n- 第一次输出时将完整答案放在 message_save 的 prompt 参数中\n- 工具返回后，直接输出答案内容给用户（不要再次调用工具）<|im_end|>\n<|im_start|>user\n请用中文简单介绍一下人工智能的历史，控制在100字以内。<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=False, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=StructuredOutputsParams(json={'properties': {'prompt': {'type': 'string'}}, 'required': ['prompt'], 'type': 'object'}, regex=None, choice=None, grammar=None, json_object=None, disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, whitespace_pattern=None, structural_tag=None, _backend=None, _backend_was_auto=False), extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None.
(APIServer pid=1) INFO 01-16 17:28:26 [async_llm.py:316] Added request chatcmpl-5fa9becc620f40588a1cee41abaddde8.
(APIServer pid=1) INFO 01-16 17:28:39 [loggers.py:127] Engine 000: Avg prompt throughput: 15.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=1) INFO 01-16 17:28:49 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=1) INFO:     192.168.1.253:39028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=1) INFO 01-16 17:28:52 [logger.py:40] Received request chatcmpl-b13a00318afa428cad4b03b6a5a37de2: prompt: '<|im_start|>system\n你是一个有用的助手。你会通过 message_save 工具保存答案到文件。\n\n工作流程：\n1. 当你第一次回答用户问题时，生成完整的中文答案，并将其作为 prompt 参数传递给 message_save 工具\n2. message_save 工具会将答案保存到文件并返回确认信息\n3. 看到工具返回后，将同样的答案直接输出给用户\n\n注意：\n- 第一次输出时将完整答案放在 message_save 的 prompt 参数中\n- 工具返回后，直接输出答案内容给用户（不要再次调用工具）<|im_end|>\n<|im_start|>user\n请用中文简单介绍一下人工智能的历史，控制在100字以内。<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>tool\n已成功保存到文件: /home/langchain_agent_logs/answer_20260116_172852.txt<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None.
(APIServer pid=1) INFO 01-16 17:28:52 [async_llm.py:316] Added request chatcmpl-b13a00318afa428cad4b03b6a5a37de2.
(APIServer pid=1) INFO 01-16 17:28:59 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671] WorkerProc hit an exception.
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671] Traceback (most recent call last):
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 666, in worker_busy_loop
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     output = func(*args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]              ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return func(*args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 447, in execute_model
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     output = self.model_runner.execute_model(scheduler_output,
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return func(*args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 2257, in execute_model
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     ) = self._prepare_inputs(scheduler_output)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 1252, in _prepare_inputs
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     attn_metadata_i = builder.build(
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                       ^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/attention/backends/flex_attention.py", line 620, in build
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     out = FlexAttentionMetadata(
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]           ^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "<string>", line 33, in __init__
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/attention/backends/flex_attention.py", line 553, in __post_init__
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     self.block_mask = self.build_block_mask()
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                       ^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/attention/backends/flex_attention.py", line 527, in build_block_mask
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return create_block_mask_compiled(
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return fn(*args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/attention/flex_attention.py", line 832, in create_block_mask
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     def create_block_mask(
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return fn(*args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1241, in forward
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return compiled_fn(full_args)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 384, in runtime_wrapper
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     all_outs = call_func_at_runtime_with_args(
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     out = normalize_as_list(f(args))
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                             ^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 750, in inner_fn
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     outs = compiled_fn(args)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 556, in wrapper
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return compiled_fn(runtime_args)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 584, in __call__
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return self.current_callable(inputs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1655, in run
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return compiled_fn(new_inputs)  # type: ignore[arg-type]
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 403, in deferred_cudagraphify
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     fn, out = cudagraphify(model, inputs, new_static_input_idxs, *args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 462, in cudagraphify
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return manager.add_function(
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 2316, in add_function
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return fn, fn(inputs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                ^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 2012, in run
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     out = self._run(new_inputs, function_id)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 2116, in _run
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return self.run_eager(new_inputs, function_id)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 2277, in run_eager
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return node.run(new_inputs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 685, in run
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     out = self.wrapped_function.model(new_inputs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/tmp/torchinductor_root/tz/ctz7lgcbvazrpqz2avwejmdqgfdso53vdqjrajinhjz4kioz3zrp.py", line 609, in call
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     triton_per_fused__to_copy_sum_8.run(buf20, buf24, 11575, triton_per_fused__to_copy_sum_8_r0_numel, stream=stream3)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1133, in run
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     self.autotune_to_one_config(*args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 960, in autotune_to_one_config
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     timings = self.benchmark_all_configs(*args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 935, in benchmark_all_configs
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     launcher: self.bench(launcher, *args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 804, in bench
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return fn(self, *args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 244, in benchmark_gpu
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     torch.cuda.synchronize()
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1085, in synchronize
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return torch._C._cuda_synchronize()
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671] torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671] Traceback (most recent call last):
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 666, in worker_busy_loop
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     output = func(*args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]              ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return func(*args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 447, in execute_model
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     output = self.model_runner.execute_model(scheduler_output,
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return func(*args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 2257, in execute_model
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     ) = self._prepare_inputs(scheduler_output)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 1252, in _prepare_inputs
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     attn_metadata_i = builder.build(
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                       ^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/attention/backends/flex_attention.py", line 620, in build
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     out = FlexAttentionMetadata(
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]           ^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "<string>", line 33, in __init__
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/attention/backends/flex_attention.py", line 553, in __post_init__
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     self.block_mask = self.build_block_mask()
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                       ^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/attention/backends/flex_attention.py", line 527, in build_block_mask
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return create_block_mask_compiled(
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return fn(*args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/attention/flex_attention.py", line 832, in create_block_mask
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     def create_block_mask(
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return fn(*args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1241, in forward
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return compiled_fn(full_args)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 384, in runtime_wrapper
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     all_outs = call_func_at_runtime_with_args(
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     out = normalize_as_list(f(args))
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                             ^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 750, in inner_fn
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     outs = compiled_fn(args)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 556, in wrapper
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return compiled_fn(runtime_args)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 584, in __call__
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return self.current_callable(inputs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1655, in run
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return compiled_fn(new_inputs)  # type: ignore[arg-type]
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 403, in deferred_cudagraphify
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     fn, out = cudagraphify(model, inputs, new_static_input_idxs, *args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 462, in cudagraphify
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return manager.add_function(
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 2316, in add_function
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return fn, fn(inputs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                ^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 2012, in run
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     out = self._run(new_inputs, function_id)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 2116, in _run
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return self.run_eager(new_inputs, function_id)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 2277, in run_eager
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return node.run(new_inputs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 685, in run
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     out = self.wrapped_function.model(new_inputs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/tmp/torchinductor_root/tz/ctz7lgcbvazrpqz2avwejmdqgfdso53vdqjrajinhjz4kioz3zrp.py", line 609, in call
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     triton_per_fused__to_copy_sum_8.run(buf20, buf24, 11575, triton_per_fused__to_copy_sum_8_r0_numel, stream=stream3)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1133, in run
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     self.autotune_to_one_config(*args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 960, in autotune_to_one_config
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     timings = self.benchmark_all_configs(*args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 935, in benchmark_all_configs
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     launcher: self.bench(launcher, *args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 804, in bench
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return fn(self, *args, **kwargs)
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 244, in benchmark_gpu
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     torch.cuda.synchronize()
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1085, in synchronize
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return torch._C._cuda_synchronize()
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671] torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]
(Worker_TP3 pid=467) ERROR 01-16 17:29:00 [multiproc_executor.py:671]
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671] WorkerProc hit an exception.
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671] Traceback (most recent call last):
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 666, in worker_busy_loop
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     output = func(*args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]              ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return func(*args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 447, in execute_model
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     output = self.model_runner.execute_model(scheduler_output,
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return func(*args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 2257, in execute_model
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     ) = self._prepare_inputs(scheduler_output)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 1252, in _prepare_inputs
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     attn_metadata_i = builder.build(
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                       ^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/attention/backends/flex_attention.py", line 620, in build
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     out = FlexAttentionMetadata(
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]           ^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "<string>", line 33, in __init__
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/attention/backends/flex_attention.py", line 553, in __post_init__
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     self.block_mask = self.build_block_mask()
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                       ^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/attention/backends/flex_attention.py", line 527, in build_block_mask
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return create_block_mask_compiled(
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return fn(*args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/attention/flex_attention.py", line 832, in create_block_mask
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     def create_block_mask(
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return fn(*args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1241, in forward
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return compiled_fn(full_args)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 384, in runtime_wrapper
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     all_outs = call_func_at_runtime_with_args(
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     out = normalize_as_list(f(args))
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                             ^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 750, in inner_fn
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     outs = compiled_fn(args)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 556, in wrapper
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return compiled_fn(runtime_args)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 584, in __call__
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return self.current_callable(inputs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1655, in run
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return compiled_fn(new_inputs)  # type: ignore[arg-type]
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 403, in deferred_cudagraphify
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     fn, out = cudagraphify(model, inputs, new_static_input_idxs, *args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 462, in cudagraphify
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return manager.add_function(
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 2316, in add_function
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return fn, fn(inputs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                ^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 2012, in run
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     out = self._run(new_inputs, function_id)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 2116, in _run
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return self.run_eager(new_inputs, function_id)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 2277, in run_eager
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return node.run(new_inputs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 685, in run
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     out = self.wrapped_function.model(new_inputs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/tmp/torchinductor_root/l7/cl7m73vvhn54522plc43zxfmc2mtp56ghhyoqbpr7ult3qd3k3id.py", line 609, in call
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     triton_per_fused__to_copy_sum_8.run(buf20, buf24, 11575, triton_per_fused__to_copy_sum_8_r0_numel, stream=stream0)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1133, in run
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     self.autotune_to_one_config(*args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 960, in autotune_to_one_config
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     timings = self.benchmark_all_configs(*args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 935, in benchmark_all_configs
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     launcher: self.bench(launcher, *args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 804, in bench
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return fn(self, *args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 244, in benchmark_gpu
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     torch.cuda.synchronize()
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1085, in synchronize
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return torch._C._cuda_synchronize()
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671] torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671] Traceback (most recent call last):
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 666, in worker_busy_loop
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     output = func(*args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]              ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return func(*args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 447, in execute_model
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     output = self.model_runner.execute_model(scheduler_output,
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return func(*args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 2257, in execute_model
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     ) = self._prepare_inputs(scheduler_output)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 1252, in _prepare_inputs
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     attn_metadata_i = builder.build(
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                       ^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/attention/backends/flex_attention.py", line 620, in build
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     out = FlexAttentionMetadata(
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]           ^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "<string>", line 33, in __init__
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/attention/backends/flex_attention.py", line 553, in __post_init__
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     self.block_mask = self.build_block_mask()
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                       ^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/attention/backends/flex_attention.py", line 527, in build_block_mask
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return create_block_mask_compiled(
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return fn(*args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/attention/flex_attention.py", line 832, in create_block_mask
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     def create_block_mask(
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return fn(*args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1241, in forward
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return compiled_fn(full_args)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 384, in runtime_wrapper
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     all_outs = call_func_at_runtime_with_args(
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     out = normalize_as_list(f(args))
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                             ^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 750, in inner_fn
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     outs = compiled_fn(args)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 556, in wrapper
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return compiled_fn(runtime_args)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 584, in __call__
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return self.current_callable(inputs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1655, in run
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return compiled_fn(new_inputs)  # type: ignore[arg-type]
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 403, in deferred_cudagraphify
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     fn, out = cudagraphify(model, inputs, new_static_input_idxs, *args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 462, in cudagraphify
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return manager.add_function(
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 2316, in add_function
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return fn, fn(inputs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]                ^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 2012, in run
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     out = self._run(new_inputs, function_id)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 2116, in _run
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return self.run_eager(new_inputs, function_id)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 2277, in run_eager
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return node.run(new_inputs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/cudagraph_trees.py", line 685, in run
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     out = self.wrapped_function.model(new_inputs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/tmp/torchinductor_root/l7/cl7m73vvhn54522plc43zxfmc2mtp56ghhyoqbpr7ult3qd3k3id.py", line 609, in call
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     triton_per_fused__to_copy_sum_8.run(buf20, buf24, 11575, triton_per_fused__to_copy_sum_8_r0_numel, stream=stream0)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1133, in run
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     self.autotune_to_one_config(*args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 960, in autotune_to_one_config
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     timings = self.benchmark_all_configs(*args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 935, in benchmark_all_configs
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     launcher: self.bench(launcher, *args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 804, in bench
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return fn(self, *args, **kwargs)
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 244, in benchmark_gpu
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     torch.cuda.synchronize()
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1085, in synchronize
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]     return torch._C._cuda_synchronize()
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671] torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]
(Worker_TP0 pid=464) ERROR 01-16 17:29:00 [multiproc_executor.py:671]
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [dump_input.py:69] Dumping input data for V1 LLM engine (v0.11.0) with config: model='/root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/eed13092ef92e448dd6875b2a00151bd3f7db0ac', speculative_config=None, tokenizer='/root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/eed13092ef92e448dd6875b2a00151bd3f7db0ac', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir='/root/.cache/huggingface', load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen2-VL-7B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null},
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [dump_input.py:76] Dumping scheduler output for model execution: SchedulerOutput(scheduled_new_reqs=[NewRequestData(req_id=chatcmpl-b13a00318afa428cad4b03b6a5a37de2,prompt_token_ids_len=191,mm_features=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[151643], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None),block_ids=([1, 2, 3, 4, 5, 6, 7, 8, 9, 16, 17, 18],),num_computed_tokens=144,lora_request=None,prompt_embeds_shape=None)], scheduled_cached_reqs=CachedRequestData(req_ids=[], resumed_from_preemption=[], new_token_ids=[], new_block_ids=[], num_computed_tokens=[]), num_scheduled_tokens={chatcmpl-b13a00318afa428cad4b03b6a5a37de2: 47}, total_num_scheduled_tokens=47, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[12], finished_req_ids=[], free_encoder_mm_hashes=[], structured_output_request_ids={}, grammar_bitmask=null, kv_connector_metadata=null)
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [dump_input.py:79] Dumping scheduler stats: SchedulerStats(num_running_reqs=1, num_waiting_reqs=0, step_counter=0, current_wave=0, kv_cache_usage=0.00012959663048761616, prefix_cache_stats=PrefixCacheStats(reset=False, requests=1, queries=191, hits=144), spec_decoding_stats=None, kv_connector_stats=None, num_corrupted_reqs=0)
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710] EngineCore encountered a fatal error.
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710] Traceback (most recent call last):
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 701, in run_engine_core
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]     engine_core.run_busy_loop()
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 728, in run_busy_loop
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]     self._process_engine_step()
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 754, in _process_engine_step
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]     outputs, model_executed = self.step_fn()
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]                               ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 284, in step
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]     model_output = self.execute_model_with_error_logging(
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 270, in execute_model_with_error_logging
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]     raise err
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 261, in execute_model_with_error_logging
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]     return model_fn(scheduler_output)
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]            ^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 181, in execute_model
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]     (output, ) = self.collective_rpc(
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]                  ^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 264, in collective_rpc
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]     result = get_response(w, dequeue_timeout,
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 248, in get_response
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710]     raise RuntimeError(
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710] RuntimeError: Worker failed with error 'CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=331) ERROR 01-16 17:29:00 [core.py:710] ', please check the stack trace above for the root cause
(Worker_TP0 pid=464) INFO 01-16 17:29:00 [multiproc_executor.py:558] Parent process exited, terminating worker
(Worker_TP0 pid=464) INFO 01-16 17:29:00 [multiproc_executor.py:599] WorkerProc shutting down.
(APIServer pid=1) ERROR 01-16 17:29:00 [async_llm.py:480] AsyncLLM output_handler failed.
(APIServer pid=1) ERROR 01-16 17:29:00 [async_llm.py:480] Traceback (most recent call last):
(APIServer pid=1) ERROR 01-16 17:29:00 [async_llm.py:480]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 439, in output_handler
(APIServer pid=1) ERROR 01-16 17:29:00 [async_llm.py:480]     outputs = await engine_core.get_output_async()
(APIServer pid=1) ERROR 01-16 17:29:00 [async_llm.py:480]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=1) ERROR 01-16 17:29:00 [async_llm.py:480]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 846, in get_output_async
(APIServer pid=1) ERROR 01-16 17:29:00 [async_llm.py:480]     raise self._format_exception(outputs) from None
(APIServer pid=1) ERROR 01-16 17:29:00 [async_llm.py:480] vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
(APIServer pid=1) INFO 01-16 17:29:00 [async_llm.py:406] Request chatcmpl-b13a00318afa428cad4b03b6a5a37de2 failed (engine dead).
(Worker_TP3 pid=467) INFO 01-16 17:29:00 [multiproc_executor.py:558] Parent process exited, terminating worker
(Worker_TP2 pid=466) INFO 01-16 17:29:00 [multiproc_executor.py:558] Parent process exited, terminating worker
(Worker_TP1 pid=465) INFO 01-16 17:29:00 [multiproc_executor.py:558] Parent process exited, terminating worker
(APIServer pid=1) INFO:     192.168.1.253:39028 - "POST /v1/chat/completions HTTP/1.1" 500 Internal Server Error
terminate called after throwing an instance of 'c10::AcceleratorError'
  what():  CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:42 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7ddd1957eeb0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x111c7 (0x7ddd1990e1c7 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0xc69358 (0x7ddcbc8fd358 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xc643a3 (0x7ddcbc8f83a3 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xc6b2dc (0x7ddcbc8ff2dc in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x434a5f (0x7ddd0c172a5f in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #6: c10::TensorImpl::~TensorImpl() + 0x9 (0x7ddd1955c179 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6c9e58 (0x7ddd0c407e58 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x6ca22d (0x7ddd0c40822d in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x12e7cd (0x7ddc176be7cd in /usr/local/lib/python3.12/dist-packages/numpy/_core/_multiarray_umath.cpython-312-x86_64-linux-gnu.so)
frame #10: VLLM::Worker_TP0() [0x53bd64]
frame #11: VLLM::Worker_TP0() [0x59bded]
frame #12: VLLM::Worker_TP0() [0x53bd64]
frame #13: VLLM::Worker_TP0() [0x59bded]
frame #14: VLLM::Worker_TP0() [0x53bd64]
frame #15: VLLM::Worker_TP0() [0x59bded]
frame #16: VLLM::Worker_TP0() [0x59bdf9]
frame #17: VLLM::Worker_TP0() [0x59bdf9]
frame #18: VLLM::Worker_TP0() [0x5646a8]
frame #19: _PyEval_EvalFrameDefault + 0xb13 (0x548323 in VLLM::Worker_TP0)
frame #20: PyEval_EvalCode + 0x99 (0x61d629 in VLLM::Worker_TP0)
frame #21: VLLM::Worker_TP0() [0x6592bb]
frame #22: VLLM::Worker_TP0() [0x654366]
frame #23: PyRun_StringFlags + 0x63 (0x650293 in VLLM::Worker_TP0)
frame #24: PyRun_SimpleStringFlags + 0x3e (0x64ff9e in VLLM::Worker_TP0)
frame #25: Py_RunMain + 0x4b2 (0x64d5a2 in VLLM::Worker_TP0)
frame #26: Py_BytesMain + 0x2d (0x6064ad in VLLM::Worker_TP0)
frame #27: <unknown function> + 0x29d90 (0x7ddd1a1d2d90 in /lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7ddd1a1d2e40 in /lib/x86_64-linux-gnu/libc.so.6)
frame #29: _start + 0x25 (0x606325 in VLLM::Worker_TP0)

terminate called after throwing an instance of 'c10::AcceleratorError'
terminate called after throwing an instance of 'c10::AcceleratorError'
  what():  CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:42 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7faa70ed9eb0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x111c7 (0x7faa70f6c1c7 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0xc69358 (0x7faa142fd358 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xc643a3 (0x7faa142f83a3 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xc6b2dc (0x7faa142ff2dc in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x434a5f (0x7faa63b72a5f in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #6: c10::TensorImpl::~TensorImpl() + 0x9 (0x7faa70eb7179 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6c9e58 (0x7faa63e07e58 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x6ca22d (0x7faa63e0822d in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x12e7cd (0x7fa96f0be7cd in /usr/local/lib/python3.12/dist-packages/numpy/_core/_multiarray_umath.cpython-312-x86_64-linux-gnu.so)
frame #10: VLLM::Worker_TP2() [0x53bd64]
frame #11: VLLM::Worker_TP2() [0x59bded]
frame #12: VLLM::Worker_TP2() [0x53bd64]
frame #13: VLLM::Worker_TP2() [0x59bded]
frame #14: VLLM::Worker_TP2() [0x53bd64]
frame #15: VLLM::Worker_TP2() [0x59bded]
frame #16: VLLM::Worker_TP2() [0x59bdf9]
frame #17: VLLM::Worker_TP2() [0x59bdf9]
frame #18: VLLM::Worker_TP2() [0x57cbd0]
frame #19: VLLM::Worker_TP2() [0x57cb91]
frame #20: VLLM::Worker_TP2() [0x57bc06]
frame #21: VLLM::Worker_TP2() [0x594bb7]
frame #22: VLLM::Worker_TP2() [0x59bca6]
frame #23: VLLM::Worker_TP2() [0x533cfa]
frame #24: VLLM::Worker_TP2() [0x659597]
frame #25: _PyEval_EvalFrameDefault + 0x9be (0x5481ce in VLLM::Worker_TP2)
frame #26: PyEval_EvalCode + 0x99 (0x61d629 in VLLM::Worker_TP2)
frame #27: VLLM::Worker_TP2() [0x6592bb]
frame #28: VLLM::Worker_TP2() [0x654366]
frame #29: PyRun_StringFlags + 0x63 (0x650293 in VLLM::Worker_TP2)
frame #30: PyRun_SimpleStringFlags + 0x3e (0x64ff9e in VLLM::Worker_TP2)
frame #31: Py_RunMain + 0x4b2 (0x64d5a2 in VLLM::Worker_TP2)
frame #32: Py_BytesMain + 0x2d (0x6064ad in VLLM::Worker_TP2)
frame #33: <unknown function> + 0x29d90 (0x7faa71b52d90 in /lib/x86_64-linux-gnu/libc.so.6)
frame #34: __libc_start_main + 0x80 (0x7faa71b52e40 in /lib/x86_64-linux-gnu/libc.so.6)
frame #35: _start + 0x25 (0x606325 in VLLM::Worker_TP2)

terminate called after throwing an instance of 'c10::AcceleratorError'
  what():  CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:42 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x72316977eeb0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x111c7 (0x723169b471c7 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0xc69358 (0x72310cafd358 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xc643a3 (0x72310caf83a3 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xc6b2dc (0x72310caff2dc in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x434a5f (0x72315c372a5f in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #6: c10::TensorImpl::~TensorImpl() + 0x9 (0x72316975c179 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6c9e58 (0x72315c607e58 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x6ca22d (0x72315c60822d in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x12e7cd (0x7230678be7cd in /usr/local/lib/python3.12/dist-packages/numpy/_core/_multiarray_umath.cpython-312-x86_64-linux-gnu.so)
frame #10: VLLM::Worker_TP1() [0x53bd64]
frame #11: VLLM::Worker_TP1() [0x59bded]
frame #12: VLLM::Worker_TP1() [0x53bd64]
frame #13: VLLM::Worker_TP1() [0x59bded]
frame #14: VLLM::Worker_TP1() [0x53bd64]
frame #15: VLLM::Worker_TP1() [0x59bded]
frame #16: VLLM::Worker_TP1() [0x59bdf9]
frame #17: VLLM::Worker_TP1() [0x59bdf9]
frame #18: VLLM::Worker_TP1() [0x57cbd0]
frame #19: VLLM::Worker_TP1() [0x57cb91]
frame #20: VLLM::Worker_TP1() [0x57bc06]
frame #21: VLLM::Worker_TP1() [0x594bb7]
frame #22: VLLM::Worker_TP1() [0x59bca6]
frame #23: VLLM::Worker_TP1() [0x533cfa]
frame #24: VLLM::Worker_TP1() [0x659597]
frame #25: _PyEval_EvalFrameDefault + 0x9be (0x5481ce in VLLM::Worker_TP1)
frame #26: PyEval_EvalCode + 0x99 (0x61d629 in VLLM::Worker_TP1)
frame #27: VLLM::Worker_TP1() [0x6592bb]
frame #28: VLLM::Worker_TP1() [0x654366]
frame #29: PyRun_StringFlags + 0x63 (0x650293 in VLLM::Worker_TP1)
frame #30: PyRun_SimpleStringFlags + 0x3e (0x64ff9e in VLLM::Worker_TP1)
frame #31: Py_RunMain + 0x4b2 (0x64d5a2 in VLLM::Worker_TP1)
frame #32: Py_BytesMain + 0x2d (0x6064ad in VLLM::Worker_TP1)
frame #33: <unknown function> + 0x29d90 (0x72316a40bd90 in /lib/x86_64-linux-gnu/libc.so.6)
frame #34: __libc_start_main + 0x80 (0x72316a40be40 in /lib/x86_64-linux-gnu/libc.so.6)
frame #35: _start + 0x25 (0x606325 in VLLM::Worker_TP1)

  what():  CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:42 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7c09e50d9eb0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x111c7 (0x7c09e516c1c7 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0xc69358 (0x7c09884fd358 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xc643a3 (0x7c09884f83a3 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xc6b2dc (0x7c09884ff2dc in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x434a5f (0x7c09d7d72a5f in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #6: c10::TensorImpl::~TensorImpl() + 0x9 (0x7c09e50b7179 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6c9e58 (0x7c09d8007e58 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x6ca22d (0x7c09d800822d in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x12e7cd (0x7c08e32be7cd in /usr/local/lib/python3.12/dist-packages/numpy/_core/_multiarray_umath.cpython-312-x86_64-linux-gnu.so)
frame #10: VLLM::Worker_TP3() [0x53bd64]
frame #11: VLLM::Worker_TP3() [0x59bded]
frame #12: VLLM::Worker_TP3() [0x53bd64]
frame #13: VLLM::Worker_TP3() [0x59bded]
frame #14: VLLM::Worker_TP3() [0x53bd64]
frame #15: VLLM::Worker_TP3() [0x59bded]
frame #16: VLLM::Worker_TP3() [0x59bdf9]
frame #17: VLLM::Worker_TP3() [0x59bdf9]
frame #18: VLLM::Worker_TP3() [0x57cbd0]
frame #19: VLLM::Worker_TP3() [0x57bc06]
frame #20: VLLM::Worker_TP3() [0x57bbff]
frame #21: VLLM::Worker_TP3() [0x57bbff]
frame #22: VLLM::Worker_TP3() [0x533cda]
frame #23: VLLM::Worker_TP3() [0x659597]
frame #24: _PyEval_EvalFrameDefault + 0x9be (0x5481ce in VLLM::Worker_TP3)
frame #25: PyEval_EvalCode + 0x99 (0x61d629 in VLLM::Worker_TP3)
frame #26: VLLM::Worker_TP3() [0x6592bb]
frame #27: VLLM::Worker_TP3() [0x654366]
frame #28: PyRun_StringFlags + 0x63 (0x650293 in VLLM::Worker_TP3)
frame #29: PyRun_SimpleStringFlags + 0x3e (0x64ff9e in VLLM::Worker_TP3)
frame #30: Py_RunMain + 0x4b2 (0x64d5a2 in VLLM::Worker_TP3)
frame #31: Py_BytesMain + 0x2d (0x6064ad in VLLM::Worker_TP3)
frame #32: <unknown function> + 0x29d90 (0x7c09e5cfdd90 in /lib/x86_64-linux-gnu/libc.so.6)
frame #33: __libc_start_main + 0x80 (0x7c09e5cfde40 in /lib/x86_64-linux-gnu/libc.so.6)
frame #34: _start + 0x25 (0x606325 in VLLM::Worker_TP3)

(APIServer pid=1) INFO:     192.168.1.253:39028 - "POST /v1/chat/completions HTTP/1.1" 500 Internal Server Error
(APIServer pid=1) INFO:     192.168.1.253:39028 - "POST /v1/chat/completions HTTP/1.1" 500 Internal Server Error
(APIServer pid=1) INFO:     Shutting down
(APIServer pid=1) INFO:     Waiting for application shutdown.
(APIServer pid=1) INFO:     Application shutdown complete.
(APIServer pid=1) INFO:     Finished server process [1]
/usr/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
