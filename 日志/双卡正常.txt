(APIServer pid=1) INFO 01-16 16:57:59 [chat_utils.py:560] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
(APIServer pid=1) INFO 01-16 16:57:59 [logger.py:40] Received request chatcmpl-79b9c398f49a4c00a008cb0dc1763ab6: prompt: '<|im_start|>system\n你是一个有用的助手。你会通过 message_save 工具保存答案到文件。\n\n工作流程：\n1. 当你第一次回答用户问题时，生成完整的中文答案，并将其作为 prompt 参数传递给 message_save 工具\n2. message_save 工具会将答案保存到文件并返回确认信息\n3. 看到工具返回后，将同样的答案直接输出给用户\n\n注意：\n- 第一次输出时将完整答案放在 message_save 的 prompt 参数中\n- 工具返回后，直接输出答案内容给用户（不要再次调用工具）<|im_end|>\n<|im_start|>user\n请用中文简单介绍一下人工智能的历史，控制在100字以内。<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=False, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=StructuredOutputsParams(json={'properties': {'prompt': {'type': 'string'}}, 'required': ['prompt'], 'type': 'object'}, regex=None, choice=None, grammar=None, json_object=None, disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, whitespace_pattern=None, structural_tag=None, _backend=None, _backend_was_auto=False), extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None.
(APIServer pid=1) INFO 01-16 16:57:59 [async_llm.py:316] Added request chatcmpl-79b9c398f49a4c00a008cb0dc1763ab6.
(APIServer pid=1) INFO 01-16 16:58:12 [loggers.py:127] Engine 000: Avg prompt throughput: 15.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=1) INFO:     192.168.1.253:40634 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=1) INFO 01-16 16:58:20 [logger.py:40] Received request chatcmpl-5c39c21e44224a3bac1e5d3cc3af73b0: prompt: '<|im_start|>system\n你是一个有用的助手。你会通过 message_save 工具保存答案到文件。\n\n工作流程：\n1. 当你第一次回答用户问题时，生成完整的中文答案，并将其作为 prompt 参数传递给 message_save 工具\n2. message_save 工具会将答案保存到文件并返回确认信息\n3. 看到工具返回后，将同样的答案直接输出给用户\n\n注意：\n- 第一次输出时将完整答案放在 message_save 的 prompt 参数中\n- 工具返回后，直接输出答案内容给用户（不要再次调用工具）<|im_end|>\n<|im_start|>user\n请用中文简单介绍一下人工智能的历史，控制在100字以内。<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>tool\n已成功保存到文件: /home/langchain_agent_logs/answer_20260116_165820.txt<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None.
(APIServer pid=1) INFO 01-16 16:58:20 [async_llm.py:316] Added request chatcmpl-5c39c21e44224a3bac1e5d3cc3af73b0.
(APIServer pid=1) INFO 01-16 16:58:22 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=1) INFO 01-16 16:58:32 [loggers.py:127] Engine 000: Avg prompt throughput: 19.1 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 42.2%
(APIServer pid=1) INFO:     192.168.1.253:40634 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=1) INFO 01-16 16:58:42 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 42.2%
(APIServer pid=1) INFO 01-16 16:58:52 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 42.2%
